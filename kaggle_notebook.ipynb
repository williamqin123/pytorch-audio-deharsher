{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579b5b12",
   "metadata": {
    "id": "t01ghZwZqXML",
    "papermill": {
     "duration": 0.007035,
     "end_time": "2024-05-25T15:01:07.918521",
     "exception": false,
     "start_time": "2024-05-25T15:01:07.911486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5431ecf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:01:07.933561Z",
     "iopub.status.busy": "2024-05-25T15:01:07.933206Z",
     "iopub.status.idle": "2024-05-25T15:01:15.169194Z",
     "shell.execute_reply": "2024-05-25T15:01:15.168422Z"
    },
    "id": "OZ9aAjhRG9YA",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "98c305e3-b912-41e8-8411-17152fccbbf5",
    "papermill": {
     "duration": 7.246038,
     "end_time": "2024-05-25T15:01:15.171593",
     "exception": false,
     "start_time": "2024-05-25T15:01:07.925555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device · count is 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, scipy as sp\n",
    "from scipy.stats import norm\n",
    "import torch, time, os, math, glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn, optim, autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "# from torchvision import datasets, transforms, models\n",
    "# import segmentation_models_pytorch as smp\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "REQUIRE_CUDA = True\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "if REQUIRE_CUDA and (not is_cuda_available):\n",
    "    raise Exception('cuda is unavailable and requested')\n",
    "device = torch.device(\"cuda\" if is_cuda_available else \"cpu\")\n",
    "print(f\"using {device} device\" + (f\" · count is {torch.cuda.device_count()}\" if is_cuda_available else ''))\n",
    "\n",
    "if is_cuda_available:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch._dynamo.reset()\n",
    "    #torch.set_default_device('cuda')\n",
    "\n",
    "#\n",
    "\n",
    "CLEAR_WEIGHT_FOLDER = False\n",
    "\n",
    "if CLEAR_WEIGHT_FOLDER:\n",
    "    import shutil\n",
    "    shutil.rmtree(\"./weight\")\n",
    "\n",
    "#\n",
    "\n",
    "if not os.path.exists('./weight'):\n",
    "    os.makedirs('./weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c3cc1",
   "metadata": {
    "id": "TjIQD5YMqgfR",
    "papermill": {
     "duration": 0.006818,
     "end_time": "2024-05-25T15:01:15.185272",
     "exception": false,
     "start_time": "2024-05-25T15:01:15.178454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# U-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756432e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:01:15.200164Z",
     "iopub.status.busy": "2024-05-25T15:01:15.199747Z",
     "iopub.status.idle": "2024-05-25T15:01:15.262798Z",
     "shell.execute_reply": "2024-05-25T15:01:15.262089Z"
    },
    "id": "wBrurFLbOgyd",
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.073087,
     "end_time": "2024-05-25T15:01:15.264774",
     "exception": false,
     "start_time": "2024-05-25T15:01:15.191687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CoordConv2d_onlyY_fixedDims(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimensions,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super(CoordConv2d_onlyY_fixedDims, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels + 2,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            dilation,\n",
    "            groups,\n",
    "            bias,\n",
    "        )\n",
    "        self.y_coords = (\n",
    "            ((torch.arange(1, 1 + dimensions[1]) + 1.0 / 2.0) / (1.0 + dimensions[1]))\n",
    "            .repeat((1, 1, dimensions[0], 1))\n",
    "            .to(device)\n",
    "        )\n",
    "        self.y_coords_log = ((11.0 + torch.log2(self.y_coords)) / 11.0).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, width, height = x.size()\n",
    "\n",
    "        # Concatenates coordinates channels to input\n",
    "        x = torch.cat(\n",
    "            [\n",
    "                x,\n",
    "                self.y_coords.expand(x.size(0), 1, width, height),\n",
    "                self.y_coords_log.expand(x.size(0), 1, width, height),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # Performs convolution\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeeplySupervizedUnet(nn.Module):\n",
    "\n",
    "    def convblock_enc(\n",
    "        self, dims, n_in_channels, n_out_channels, permits_slimming=False\n",
    "    ):\n",
    "        if not permits_slimming:\n",
    "            assert n_in_channels <= n_out_channels\n",
    "        # https://debuggercafe.com/unet-from-scratch-using-pytorch\n",
    "        \"\"\"\n",
    "        In the original paper implementation, the convolution operations were\n",
    "        not padded but we are padding them here. This is because, we need the\n",
    "        output result size to be same as input size.\n",
    "        \"\"\"\n",
    "        biggest_n_channels = max(n_in_channels, n_out_channels)\n",
    "        conv_op = nn.Sequential(\n",
    "            CoordConv2d_onlyY_fixedDims(\n",
    "                dims,\n",
    "                n_in_channels,\n",
    "                biggest_n_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(biggest_n_channels),\n",
    "            nn.Mish(inplace=True),\n",
    "            CoordConv2d_onlyY_fixedDims(\n",
    "                dims,\n",
    "                biggest_n_channels,\n",
    "                n_out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(n_out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        return conv_op\n",
    "\n",
    "    def convblock_dec(\n",
    "        self, n_in_channels, n_out_channels, permits_thicken=False, dropout_p=0.0\n",
    "    ):\n",
    "        if not permits_thicken:\n",
    "            assert n_out_channels <= n_in_channels\n",
    "        # https://debuggercafe.com/unet-from-scratch-using-pytorch\n",
    "        \"\"\"\n",
    "        In the original paper implementation, the convolution operations were\n",
    "        not padded but we are padding them here. This is because, we need the\n",
    "        output result size to be same as input size.\n",
    "        \"\"\"\n",
    "        mean_n_channels = (n_in_channels + n_out_channels) // 2\n",
    "        conv_op = nn.Sequential(\n",
    "            (nn.Dropout2d(p=dropout_p) if dropout_p > 0 else nn.Identity()),\n",
    "            nn.Conv2d(\n",
    "                n_in_channels, mean_n_channels, kernel_size=3, padding=1, bias=True\n",
    "            ),\n",
    "            nn.Mish(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                mean_n_channels, n_out_channels, kernel_size=3, padding=1, bias=True\n",
    "            ),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        return conv_op\n",
    "\n",
    "    def downsampler(self, n_channels, ratios):\n",
    "        r_x, r_y = ratios\n",
    "        assert r_x == int(r_x) and r_y == int(r_y)\n",
    "\n",
    "        # depthwise-seperable\n",
    "        conv_op = nn.Sequential(\n",
    "            (\n",
    "                nn.Conv2d(\n",
    "                    n_channels,\n",
    "                    n_channels,\n",
    "                    kernel_size=(1, 1 + 2 * (r_y - 1)),\n",
    "                    padding=(0, r_y - 1),\n",
    "                    bias=True,\n",
    "                    groups=n_channels,\n",
    "                )\n",
    "                if r_x != 1\n",
    "                else nn.Identity()\n",
    "            ),\n",
    "            (\n",
    "                nn.Conv2d(\n",
    "                    n_channels,\n",
    "                    n_channels,\n",
    "                    kernel_size=(1 + 2 * (r_x - 1), 1),\n",
    "                    padding=(r_x - 1, 0),\n",
    "                    bias=True,\n",
    "                    groups=n_channels,\n",
    "                )\n",
    "                if r_y != 1\n",
    "                else nn.Identity()\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                n_channels,\n",
    "                n_channels,\n",
    "                kernel_size=1,\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.MaxPool2d(kernel_size=(r_x, r_y), stride=(r_x, r_y)),\n",
    "            # nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        return conv_op\n",
    "\n",
    "    def upsampler(self, n_channels, ratios):\n",
    "        r_x, r_y = ratios\n",
    "        assert r_x == int(r_x) and r_y == int(r_y)\n",
    "        conv_op = nn.Sequential(\n",
    "            nn.Upsample(\n",
    "                scale_factor=ratios,\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            ),\n",
    "            (\n",
    "                nn.Conv2d(\n",
    "                    n_channels,\n",
    "                    n_channels,\n",
    "                    kernel_size=(1 + 2 * (r_x - 1), 1),\n",
    "                    padding=(r_x - 1, 0),\n",
    "                    bias=True,\n",
    "                    groups=n_channels,\n",
    "                )\n",
    "                if r_x != 1\n",
    "                else nn.Identity()\n",
    "            ),\n",
    "            (\n",
    "                nn.Conv2d(\n",
    "                    n_channels,\n",
    "                    n_channels,\n",
    "                    kernel_size=(1, 1 + 2 * (r_y - 1)),\n",
    "                    padding=(0, r_y - 1),\n",
    "                    bias=True,\n",
    "                    groups=n_channels,\n",
    "                )\n",
    "                if r_y != 1\n",
    "                else nn.Identity()\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                n_channels,\n",
    "                n_channels,\n",
    "                kernel_size=1,\n",
    "                bias=True,\n",
    "            ),\n",
    "            # nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        return conv_op\n",
    "\n",
    "    def exporter(self, n_in_channels, n_intermediate_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                n_in_channels,\n",
    "                n_intermediate_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                n_intermediate_channels,\n",
    "                1,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def __init__(self):\n",
    "        INPUTS_PATHS_DIMS = [\n",
    "            [(64, 256), (32, 256)],\n",
    "            [(32, 512), (16, 512)],\n",
    "            [(16, 1024), (16, 512)],\n",
    "        ]\n",
    "        N_INPUT_PATHS = len(INPUTS_PATHS_DIMS)\n",
    "        DIMS_AT_JOIN = (16, 256)\n",
    "        DIMS_OUTS = [DIMS_AT_JOIN, (32, 512), (32, 1024)]\n",
    "        DIMS_SMALLEST = (2, 4)\n",
    "        N_TIER_1_FEATURE_MAPS = 32\n",
    "        N_TIER_2_FEATURE_MAPS = 64\n",
    "        N_TIER_3_FEATURE_MAPS = 96\n",
    "        N_TIER_4_FEATURE_MAPS = 128\n",
    "        N_TIER_5_FEATURE_MAPS = 256\n",
    "        N_TIER_6_FEATURE_MAPS = 384\n",
    "        N_FC_HIDDEN_LAYER_NODES = 512\n",
    "        N_FC_OUTPUTS = 24\n",
    "        N_TIER_1_PRE_DIMENSIONALITY_REDUC_FEATURE_MAPS = 16\n",
    "\n",
    "        super(DeeplySupervizedUnet, self).__init__()\n",
    "\n",
    "        self.input_1_path = nn.ModuleList(\n",
    "            [\n",
    "                self.convblock_enc(INPUTS_PATHS_DIMS[0][0], 1, N_TIER_1_FEATURE_MAPS),\n",
    "                self.downsampler(N_TIER_1_FEATURE_MAPS, (2, 1)),\n",
    "                self.convblock_enc(\n",
    "                    INPUTS_PATHS_DIMS[0][1],\n",
    "                    N_TIER_1_FEATURE_MAPS,\n",
    "                    N_TIER_2_FEATURE_MAPS,\n",
    "                ),\n",
    "                self.downsampler(N_TIER_2_FEATURE_MAPS, (2, 1)),\n",
    "            ]\n",
    "        )\n",
    "        self.input_2_path = nn.ModuleList(\n",
    "            [\n",
    "                self.convblock_enc(INPUTS_PATHS_DIMS[1][0], 1, N_TIER_1_FEATURE_MAPS),\n",
    "                self.downsampler(N_TIER_1_FEATURE_MAPS, (2, 1)),\n",
    "                self.convblock_enc(\n",
    "                    INPUTS_PATHS_DIMS[1][1],\n",
    "                    N_TIER_1_FEATURE_MAPS,\n",
    "                    N_TIER_2_FEATURE_MAPS,\n",
    "                ),\n",
    "                self.downsampler(N_TIER_2_FEATURE_MAPS, (1, 2)),\n",
    "            ]\n",
    "        )\n",
    "        self.input_3_path = nn.ModuleList(\n",
    "            [\n",
    "                self.convblock_enc(INPUTS_PATHS_DIMS[2][0], 1, N_TIER_1_FEATURE_MAPS),\n",
    "                self.downsampler(N_TIER_1_FEATURE_MAPS, (1, 2)),\n",
    "                self.convblock_enc(\n",
    "                    INPUTS_PATHS_DIMS[2][1],\n",
    "                    N_TIER_1_FEATURE_MAPS,\n",
    "                    N_TIER_2_FEATURE_MAPS,\n",
    "                ),\n",
    "                self.downsampler(N_TIER_2_FEATURE_MAPS, (1, 2)),\n",
    "            ]\n",
    "        )\n",
    "        self.joined_path_encode = nn.ModuleList(\n",
    "            [\n",
    "                self.convblock_enc(\n",
    "                    DIMS_AT_JOIN,\n",
    "                    N_INPUT_PATHS * N_TIER_2_FEATURE_MAPS,\n",
    "                    N_TIER_3_FEATURE_MAPS,\n",
    "                    permits_slimming=True,\n",
    "                ),\n",
    "                self.downsampler(N_TIER_3_FEATURE_MAPS, (2, 4)),  # 8 x 64\n",
    "                self.convblock_enc(\n",
    "                    (8, 64),\n",
    "                    N_TIER_3_FEATURE_MAPS,\n",
    "                    N_TIER_4_FEATURE_MAPS,\n",
    "                ),\n",
    "                self.downsampler(N_TIER_4_FEATURE_MAPS, (2, 4)),  # 4 x 16\n",
    "                self.convblock_enc(\n",
    "                    (4, 16),\n",
    "                    N_TIER_4_FEATURE_MAPS,\n",
    "                    N_TIER_5_FEATURE_MAPS,\n",
    "                ),\n",
    "                self.downsampler(N_TIER_5_FEATURE_MAPS, (2, 4)),  # 2 x 4\n",
    "                self.convblock_enc(\n",
    "                    DIMS_SMALLEST,\n",
    "                    N_TIER_5_FEATURE_MAPS,\n",
    "                    N_TIER_6_FEATURE_MAPS,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.skip_inp_1_tier_0_scaler = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=(2, 1), stride=(2, 1)),\n",
    "            self.upsampler(1, (1, 4)),\n",
    "        )\n",
    "        self.skip_inp_2_tier_0_scaler = self.upsampler(1, (1, 2))\n",
    "        self.skip_inp_3_tier_0_scaler = self.upsampler(1, (2, 1))\n",
    "        self.skip_inp_1_tier_1_scaler = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=(2, 1), stride=(2, 1)),\n",
    "            self.upsampler(N_TIER_1_FEATURE_MAPS, (1, 4)),\n",
    "        )\n",
    "        self.skip_inp_2_tier_1_scaler = self.upsampler(N_TIER_1_FEATURE_MAPS, (1, 2))\n",
    "        self.skip_inp_3_tier_1_scaler = self.upsampler(N_TIER_1_FEATURE_MAPS, (2, 1))\n",
    "        self.skip_inp_1_tier_2_scaler = self.upsampler(N_TIER_2_FEATURE_MAPS, (1, 2))\n",
    "        self.skip_inp_2_tier_2_scaler = self.upsampler(N_TIER_2_FEATURE_MAPS, (2, 1))\n",
    "        self.skip_inp_3_tier_2_scaler = self.upsampler(N_TIER_2_FEATURE_MAPS, (2, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),  # default : start_dim=1\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(\n",
    "                N_TIER_6_FEATURE_MAPS * DIMS_SMALLEST[0] * DIMS_SMALLEST[1],\n",
    "                N_FC_HIDDEN_LAYER_NODES,\n",
    "            ),\n",
    "            nn.Mish(True),\n",
    "            nn.Dropout(p=0.05),\n",
    "            nn.Linear(\n",
    "                N_FC_HIDDEN_LAYER_NODES,\n",
    "                N_FC_OUTPUTS,\n",
    "            ),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.01),\n",
    "        )  # so the expanding path can factor in hollistic information about the image as a whole and not just local patterns\n",
    "        self.path_decode = nn.ModuleList(\n",
    "            [\n",
    "                self.upsampler(N_TIER_6_FEATURE_MAPS, (2, 4)),  # 4 x 16\n",
    "                self.convblock_dec(\n",
    "                    N_FC_OUTPUTS + N_TIER_6_FEATURE_MAPS + N_TIER_5_FEATURE_MAPS,\n",
    "                    N_TIER_5_FEATURE_MAPS,\n",
    "                    dropout_p=0.1,\n",
    "                ),\n",
    "                self.upsampler(N_TIER_5_FEATURE_MAPS, (2, 4)),  # 8 x 64\n",
    "                self.convblock_dec(\n",
    "                    N_FC_OUTPUTS + N_TIER_5_FEATURE_MAPS + N_TIER_4_FEATURE_MAPS,\n",
    "                    N_TIER_4_FEATURE_MAPS,\n",
    "                    dropout_p=0.1,\n",
    "                ),\n",
    "                self.upsampler(N_TIER_4_FEATURE_MAPS, (2, 4)),  # 16 x 256\n",
    "                self.convblock_dec(\n",
    "                    N_FC_OUTPUTS + N_TIER_4_FEATURE_MAPS + N_TIER_3_FEATURE_MAPS,\n",
    "                    N_TIER_3_FEATURE_MAPS,\n",
    "                    dropout_p=0.1,\n",
    "                ),\n",
    "                self.upsampler(N_TIER_3_FEATURE_MAPS, (2, 2)),  # 32 x 512\n",
    "                self.convblock_dec(\n",
    "                    N_FC_OUTPUTS\n",
    "                    + N_TIER_3_FEATURE_MAPS\n",
    "                    + N_INPUT_PATHS * N_TIER_2_FEATURE_MAPS,\n",
    "                    N_TIER_2_FEATURE_MAPS,\n",
    "                    dropout_p=0.05,\n",
    "                ),\n",
    "                self.upsampler(N_TIER_2_FEATURE_MAPS, (1, 2)),  # 32 x 1024\n",
    "                self.convblock_dec(\n",
    "                    N_FC_OUTPUTS\n",
    "                    + N_TIER_2_FEATURE_MAPS\n",
    "                    + N_INPUT_PATHS * N_TIER_1_FEATURE_MAPS,\n",
    "                    N_TIER_1_FEATURE_MAPS,\n",
    "                    dropout_p=0.05,\n",
    "                ),\n",
    "                self.exporter(\n",
    "                    N_FC_OUTPUTS + N_TIER_1_FEATURE_MAPS + N_INPUT_PATHS,\n",
    "                    N_TIER_1_PRE_DIMENSIONALITY_REDUC_FEATURE_MAPS,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.deep_superv_out_t4 = self.exporter(\n",
    "            N_TIER_4_FEATURE_MAPS,\n",
    "            N_TIER_1_PRE_DIMENSIONALITY_REDUC_FEATURE_MAPS,\n",
    "        )\n",
    "        self.deep_superv_out_t3 = self.exporter(\n",
    "            N_TIER_3_FEATURE_MAPS,\n",
    "            N_TIER_1_PRE_DIMENSIONALITY_REDUC_FEATURE_MAPS,\n",
    "        )\n",
    "        self.deep_superv_out_t2 = self.exporter(\n",
    "            N_TIER_2_FEATURE_MAPS,\n",
    "            N_TIER_1_PRE_DIMENSIONALITY_REDUC_FEATURE_MAPS,\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3, do_deep_supervision=False):\n",
    "        skip_t_0 = torch.cat(\n",
    "            [\n",
    "                self.skip_inp_1_tier_0_scaler(x1),\n",
    "                self.skip_inp_2_tier_0_scaler(x2),\n",
    "                self.skip_inp_3_tier_0_scaler(x3),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        in_path_1 = self.input_1_path[0](x1)\n",
    "        skip_1_1 = self.skip_inp_1_tier_1_scaler(in_path_1)\n",
    "        in_path_1 = self.input_1_path[1](in_path_1)\n",
    "        in_path_1 = self.input_1_path[2](in_path_1)\n",
    "        skip_1_2 = self.skip_inp_1_tier_2_scaler(in_path_1)\n",
    "        in_path_1 = self.input_1_path[3](in_path_1)\n",
    "\n",
    "        in_path_2 = self.input_2_path[0](x2)\n",
    "        skip_2_1 = self.skip_inp_2_tier_1_scaler(in_path_2)\n",
    "        in_path_2 = self.input_2_path[1](in_path_2)\n",
    "        in_path_2 = self.input_2_path[2](in_path_2)\n",
    "        skip_2_2 = self.skip_inp_2_tier_2_scaler(in_path_2)\n",
    "        in_path_2 = self.input_2_path[3](in_path_2)\n",
    "\n",
    "        in_path_3 = self.input_3_path[0](x3)\n",
    "        skip_3_1 = self.skip_inp_3_tier_1_scaler(in_path_3)\n",
    "        in_path_3 = self.input_3_path[1](in_path_3)\n",
    "        in_path_3 = self.input_3_path[2](in_path_3)\n",
    "        skip_3_2 = self.skip_inp_3_tier_2_scaler(in_path_3)\n",
    "        in_path_3 = self.input_3_path[3](in_path_3)\n",
    "\n",
    "        enc = self.joined_path_encode[0](\n",
    "            torch.cat([in_path_1, in_path_2, in_path_3], dim=1)\n",
    "        )\n",
    "        skip_t_3 = enc\n",
    "        enc = self.joined_path_encode[1](enc)\n",
    "        enc = self.joined_path_encode[2](enc)\n",
    "        skip_t_4 = enc\n",
    "        enc = self.joined_path_encode[3](enc)\n",
    "        enc = self.joined_path_encode[4](enc)\n",
    "        skip_t_5 = enc\n",
    "        enc = self.joined_path_encode[5](enc)\n",
    "        enc = self.joined_path_encode[6](enc)\n",
    "\n",
    "        fc = self.fc(enc)\n",
    "        fc_delinearized = torch.unsqueeze(torch.unsqueeze(fc, 2), 3)\n",
    "\n",
    "        dec = self.path_decode[0](enc)\n",
    "        dec = self.path_decode[1](\n",
    "            torch.cat([fc_delinearized.repeat((1, 1, 4, 16)), skip_t_5, dec], 1)\n",
    "        )\n",
    "        dec = self.path_decode[2](dec)\n",
    "        dec = self.path_decode[3](\n",
    "            torch.cat([fc_delinearized.repeat((1, 1, 8, 64)), skip_t_4, dec], 1)\n",
    "        )\n",
    "        deep_superv_out_t4 = (\n",
    "            self.deep_superv_out_t4(dec) if do_deep_supervision else None\n",
    "        )\n",
    "        dec = self.path_decode[4](dec)\n",
    "        dec = self.path_decode[5](\n",
    "            torch.cat([fc_delinearized.repeat((1, 1, 16, 256)), skip_t_3, dec], 1)\n",
    "        )\n",
    "        deep_superv_out_t3 = (\n",
    "            self.deep_superv_out_t3(dec) if do_deep_supervision else None\n",
    "        )\n",
    "        dec = self.path_decode[6](dec)\n",
    "        dec = self.path_decode[7](\n",
    "            torch.cat(\n",
    "                [\n",
    "                    fc_delinearized.repeat((1, 1, 32, 512)),\n",
    "                    skip_1_2,\n",
    "                    skip_2_2,\n",
    "                    skip_3_2,\n",
    "                    dec,\n",
    "                ],\n",
    "                1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        deep_superv_out_t2 = (\n",
    "            self.deep_superv_out_t2(dec) if do_deep_supervision else None\n",
    "        )\n",
    "        dec = self.path_decode[8](dec)\n",
    "        dec = self.path_decode[9](\n",
    "            torch.cat(\n",
    "                [\n",
    "                    fc_delinearized.repeat((1, 1, 32, 1024)),\n",
    "                    skip_1_1,\n",
    "                    skip_2_1,\n",
    "                    skip_3_1,\n",
    "                    dec,\n",
    "                ],\n",
    "                1,\n",
    "            )\n",
    "        )\n",
    "        dec = self.path_decode[10](\n",
    "            torch.cat([fc_delinearized.repeat((1, 1, 32, 1024)), skip_t_0, dec], 1)\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            (deep_superv_out_t4, deep_superv_out_t3, deep_superv_out_t2, dec)\n",
    "            if do_deep_supervision\n",
    "            else dec\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e83ae",
   "metadata": {
    "id": "Vhblc41Kqkr6",
    "papermill": {
     "duration": 0.006683,
     "end_time": "2024-05-25T15:01:15.278035",
     "exception": false,
     "start_time": "2024-05-25T15:01:15.271352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loss Function\n",
    "\n",
    "~~Combination of MSE in decibels and MSE in linear amplitude~~\n",
    "\n",
    "Volume-Weighted Mean of Squared ( Error Function that Penalizes Same-Sign Undershooting Less than It Penalizes Overshooting and Opposite-Sign Predictions )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e7e48c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:01:15.292343Z",
     "iopub.status.busy": "2024-05-25T15:01:15.292053Z",
     "iopub.status.idle": "2024-05-25T15:01:15.305450Z",
     "shell.execute_reply": "2024-05-25T15:01:15.304415Z"
    },
    "id": "hmQX-RmBo-7b",
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.022941,
     "end_time": "2024-05-25T15:01:15.307434",
     "exception": false,
     "start_time": "2024-05-25T15:01:15.284493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomMSELoss, self).__init__()\n",
    "        self.softplus_c = 50\n",
    "        self.vol_ref_dB = 100\n",
    "        self.epsilon = 0.000001\n",
    "        self.fade_to_unabridge_error_mag_thresh_dB = 0.01\n",
    "\n",
    "    def softplus(self, x, c):\n",
    "        # https://stackoverflow.com/a/60908241/4356188\n",
    "        return torch.where(x < 50, c * torch.log1p(torch.exp(x / c)), x)\n",
    "\n",
    "    def sign_continuous_differentiable(self, x):\n",
    "        return torch.tanh(1000000 * x)\n",
    "\n",
    "    def modified_absolute_err(self, predictions, targets):\n",
    "        d = lambda a: torch.clamp(torch.tanh(-a), min=0) * (\n",
    "            (1 + torch.tanh(2 * (1 / 2 + a))) / 2\n",
    "        )\n",
    "        f = lambda r: torch.abs(r) - d(r) / 2\n",
    "\n",
    "        abs_targets = torch.abs(targets)\n",
    "        # print(torch.min(abs_targets[abs_targets > 0]))\n",
    "        # f_val = torch.where(abs_targets > 0, f((predictions - targets) / targets), 0)\n",
    "\n",
    "        mod_err = (self.epsilon + abs_targets) * f(\n",
    "            self.sign_continuous_differentiable(targets)\n",
    "            * (predictions - targets)\n",
    "            / (self.epsilon + abs_targets)\n",
    "        )\n",
    "        pure_err = torch.abs(predictions - targets)\n",
    "        tinyness = torch.pow(self.fade_to_unabridge_error_mag_thresh_dB, abs_targets)\n",
    "\n",
    "        return tinyness * pure_err + (1 - tinyness) * mod_err\n",
    "        \"\"\"\n",
    "        torch.where(\n",
    "            torch.isfinite(f_val),\n",
    "            abs_targets * f_val,\n",
    "            torch.abs(predictions),\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "    def weighted_mse(self, predictions, targets, weights):\n",
    "        # different weights within each train item\n",
    "        # all train items in batch have same weight\n",
    "        return torch.mean(\n",
    "            torch.sum(\n",
    "                weights * (self.modified_absolute_err(predictions, targets) ** 2),\n",
    "                dim=(1, 2, 3),\n",
    "            )\n",
    "            / torch.sum(weights, dim=(1, 2, 3))\n",
    "        )\n",
    "\n",
    "    def calc_weights(self, base, residuals_predicted, residuals_targets):\n",
    "        base_predicted = self.softplus(base + residuals_predicted, self.softplus_c)\n",
    "        base_targets = self.softplus(base + residuals_targets, self.softplus_c)\n",
    "        max_vol = torch.maximum(base_predicted, base_targets) / self.vol_ref_dB\n",
    "        return max_vol\n",
    "\n",
    "    def forward(self, inputs_2048, predictions, targets):\n",
    "        return self.weighted_mse(\n",
    "            predictions, targets, self.calc_weights(inputs_2048, predictions, targets)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42358dd",
   "metadata": {
    "id": "FxPmA6uCqtAS",
    "papermill": {
     "duration": 0.006445,
     "end_time": "2024-05-25T15:01:15.320722",
     "exception": false,
     "start_time": "2024-05-25T15:01:15.314277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trainer + Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "088e590e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:01:15.335946Z",
     "iopub.status.busy": "2024-05-25T15:01:15.335588Z",
     "iopub.status.idle": "2024-05-25T15:01:15.371589Z",
     "shell.execute_reply": "2024-05-25T15:01:15.370819Z"
    },
    "id": "ShF5TZutOt8B",
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.046215,
     "end_time": "2024-05-25T15:01:15.373531",
     "exception": false,
     "start_time": "2024-05-25T15:01:15.327316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_STORE_PATH = \"./weight/\"\n",
    "NEWEST_MODEL_SAVE_FILE_NAME = \"last.pth\"\n",
    "BEST_MODEL_SAVE_FILE_NAME = \"best.pth\"\n",
    "CHECKPOINT_FILE_PATH = MODEL_STORE_PATH + NEWEST_MODEL_SAVE_FILE_NAME\n",
    "\n",
    "MAX_SESSION_HOURS = 12\n",
    "def train_and_val(epochs, model, train_loader, len_train, val_loader, len_val, criterion, optimizer, scheduler,\n",
    "                  intermediates_contributions_decays_per_epoch,\n",
    "                  device, time_limit_hrs=None, early_stop_thresh_proportion_epoch=0.25, use_amp=True):\n",
    "    \n",
    "    assert ( not (time_limit_hrs is None) ) and time_limit_hrs < MAX_SESSION_HOURS\n",
    "    time_limit_s = time_limit_hrs * 60 ** 2\n",
    "    \n",
    "    min_epochs_train_until_consider_early_stop = math.ceil(early_stop_thresh_proportion_epoch * epochs / 2)\n",
    "\n",
    "    n_train_batches = len(train_loader)\n",
    "    n_val_batches = len(val_loader)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    \n",
    "    per_epoch_metrics = {\n",
    "        'lr':[],\n",
    "        'train_loss':[],\n",
    "        'val_loss':[],\n",
    "        'val_means_of_items_peak_errors':[],\n",
    "        'val_means_of_batches_peak_errors':[],\n",
    "        'val_epochs_peak_errors':[],\n",
    "        'deep_weights':[\n",
    "            [],[],[],\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    lowest_val_loss = None\n",
    "    recent_i_epoch_at_lowest_val_loss = 0\n",
    "\n",
    "    #model = model.to(device)\n",
    "    fit_time = time.time()\n",
    "    time_to_halt = fit_time + time_limit_s\n",
    "    dur_last_epoch_s = 0\n",
    "    for i_e in range(epochs):\n",
    "        \n",
    "        print('┅┅┅┅┅┅┅┅┅┅')\n",
    "        \n",
    "        n_e = 1 + i_e\n",
    "        since = time.time()\n",
    "        \n",
    "        running_loss = 0\n",
    "        running_lr = 0\n",
    "        running_deep_loss_weights = torch.zeros(len(intermediates_contributions_decays_per_epoch)).to(device)\n",
    "        \n",
    "        \"\"\"\n",
    "        with torch.profiler.profile(\n",
    "            schedule=torch.profiler.schedule(\n",
    "                wait=1,\n",
    "                warmup=1,\n",
    "                active=5,\n",
    "                repeat=1),\n",
    "            on_trace_ready=torch.profiler.tensorboard_trace_handler('/kaggle/working/profiler'),\n",
    "            profile_memory=True,\n",
    "            #with_stack=True,\n",
    "        ) as profiler:\n",
    "        \"\"\"\n",
    "        with tqdm(total=n_train_batches,disable=IS_KERNEL_OFFLINE_RUN) as pbar:\n",
    "            for (resonant1,resonant2,resonant3), (deresonated_deep4,deresonated_deep3,deresonated_deep2,deresonated_the) in train_loader:\n",
    "                \n",
    "                scheduler_epochs_elapsed = scheduler.last_epoch / n_train_batches\n",
    "                intermediates_contributions_per_epoch = intermediates_contributions_decays_per_epoch ** scheduler_epochs_elapsed\n",
    "                running_deep_loss_weights += torch.from_numpy(intermediates_contributions_per_epoch).to(device)\n",
    "                \n",
    "                resonant1,resonant2,resonant3,deresonated_deep4,deresonated_deep3,deresonated_deep2,deresonated_the = (resonant1.to(device),\n",
    "                                                                                                     resonant2.to(device),\n",
    "                                                                                                     resonant3.to(device),\n",
    "                                                                                                     deresonated_deep4.to(device),\n",
    "                                                                                                     deresonated_deep3.to(device),\n",
    "                                                                                                     deresonated_deep2.to(device),\n",
    "                                                                                                     deresonated_the.to(device)\n",
    "                                                                                                     )\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                model.train()\n",
    "                \n",
    "                with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "\n",
    "                    # forward\n",
    "                    output_hidden_tier4,output_hidden_tier3,output_hidden_tier2,output_proper = model(resonant1,resonant2,resonant3,do_deep_supervision=True)\n",
    "                    \n",
    "                    # loss\n",
    "                    # resonant_middle_channel = resonant2 #resonant[:,1:2]\n",
    "                    loss_out_layer = criterion(output_proper,deresonated_the)\n",
    "                    loss_intermediates = (\n",
    "                        intermediates_contributions_per_epoch[0] * criterion(output_hidden_tier4,deresonated_deep4) +\n",
    "                        intermediates_contributions_per_epoch[1] * criterion(output_hidden_tier3,deresonated_deep3) +\n",
    "                        intermediates_contributions_per_epoch[2] * criterion(output_hidden_tier2,deresonated_deep2)\n",
    "                    )\n",
    "                    loss = loss_out_layer + loss_intermediates #criterion(resonant_middle_channel, output, deresonated)\n",
    "                \n",
    "                '''\n",
    "                # backward\n",
    "                loss.backward()\n",
    "                #torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "                optimizer.step()  # update weight\n",
    "                '''\n",
    "                # Scales loss. Calls ``backward()`` on scaled loss to create scaled gradients.\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # ``scaler.step()`` first unscales the gradients of the optimizer's assigned parameters.\n",
    "                # If these gradients do not contain ``inf``s or ``NaN``s, optimizer.step() is then called,\n",
    "                # otherwise, optimizer.step() is skipped.\n",
    "                scaler.step(optimizer)\n",
    "\n",
    "                # Updates the scale for next iteration.\n",
    "                scaler.update()\n",
    "                \n",
    "                running_lr += np.mean(scheduler.get_last_lr())\n",
    "                scheduler.step()\n",
    "\n",
    "                running_loss += loss_out_layer.item()\n",
    "                \n",
    "                pbar.update(1)\n",
    "                #profiler.step()\n",
    "                \n",
    "        train_loader.dataset.epoch() # requires non-persistent DataLoader-workers\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = 0\n",
    "        val_peaks = 0\n",
    "        val_batchwide_peaks = 0\n",
    "        val_epochwide_peak = 0\n",
    "        # validation loop\n",
    "        with torch.no_grad():\n",
    "            with tqdm(total=n_val_batches,disable=IS_KERNEL_OFFLINE_RUN) as pb:\n",
    "                for (resonant1,resonant2,resonant3), (deresonated_deep4,deresonated_deep3,deresonated_deep2,deresonated_the) in val_loader:\n",
    "                    \n",
    "                    resonant1,resonant2,resonant3,deresonated_the = resonant1.to(device),resonant2.to(device),resonant3.to(device),deresonated_the.to(device)\n",
    "                    \n",
    "                    with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "\n",
    "                        # forward\n",
    "                        output_proper = model(resonant1,resonant2,resonant3,do_deep_supervision=False)\n",
    "\n",
    "                        # loss\n",
    "                        # resonant_middle_channel = resonant2 #resonant[:,1:2]\n",
    "                        loss_out_layer = criterion(output_proper,deresonated_the)\n",
    "\n",
    "                    val_losses += loss_out_layer.item()\n",
    "                    \n",
    "                    # metric\n",
    "                    peak_per_item = torch.amax(torch.abs(output_proper - deresonated_the),dim=(1,2,3)) # shape should be 1d = batch size\n",
    "                    val_peaks += torch.mean(peak_per_item).item()\n",
    "                    peak_this_batch = torch.max(peak_per_item).item()\n",
    "                    val_batchwide_peaks += peak_this_batch\n",
    "                    if peak_this_batch > val_epochwide_peak:\n",
    "                        val_epochwide_peak = peak_this_batch\n",
    "                    \n",
    "                    pb.update(1)\n",
    "\n",
    "            # calculates mean of batches\n",
    "            current_train_loss = running_loss / len_train\n",
    "            current_val_loss = val_losses / len_val\n",
    "            current_epoch_mean_lr = running_lr / len_train\n",
    "            if math.isnan(current_train_loss) or math.isnan(current_val_loss):\n",
    "                raise Exception(\"💥 NaN loss 💥\")\n",
    "            per_epoch_metrics['train_loss'].append(current_train_loss)\n",
    "            per_epoch_metrics['val_loss'].append(current_val_loss)\n",
    "            per_epoch_metrics['lr'].append(current_epoch_mean_lr)\n",
    "            for i in range(len(intermediates_contributions_decays_per_epoch)):\n",
    "                per_epoch_metrics['deep_weights'][i].append(running_deep_loss_weights[i].item() / n_train_batches)\n",
    "            \n",
    "            val_itemwide_peaks_mean = val_peaks / n_val_batches\n",
    "            val_batchwide_peaks_mean = val_batchwide_peaks / n_val_batches\n",
    "            per_epoch_metrics['val_means_of_items_peak_errors'].append(val_itemwide_peaks_mean)\n",
    "            per_epoch_metrics['val_means_of_batches_peak_errors'].append(val_batchwide_peaks_mean)\n",
    "            per_epoch_metrics['val_epochs_peak_errors'].append(val_epochwide_peak)\n",
    "\n",
    "            # saves progress\n",
    "            info_to_save = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }\n",
    "            torch.save(info_to_save, CHECKPOINT_FILE_PATH)\n",
    "            if lowest_val_loss is None or current_val_loss < lowest_val_loss:\n",
    "                recent_i_epoch_at_lowest_val_loss = i_e\n",
    "                lowest_val_loss = current_val_loss\n",
    "                torch.save(info_to_save, MODEL_STORE_PATH + BEST_MODEL_SAVE_FILE_NAME)\n",
    "            \n",
    "            allotted_seconds_left = time_to_halt - time.time()\n",
    "            dur_this_epoch_s = time.time() - since\n",
    "            print(\"Epoch:{}/{}..\".format(n_e, epochs),\n",
    "                  \"Train Loss: {:.5f}..\".format(current_train_loss),\n",
    "                  \"Val Loss: {:.5f}..\".format(current_val_loss),\n",
    "                  \"LR: {:.9f}..\".format(current_epoch_mean_lr),\n",
    "                  \"Epoch’s Duration: {:.3f} s..\".format(dur_this_epoch_s),\n",
    "                  \"Time Left: {:.3f} m\".format(allotted_seconds_left / 60),\n",
    "            )\n",
    "            print(\n",
    "                \"Val Epoch-Mean Batch-Mean Max Deviation in Decibels: {:.4f}..\".format(val_itemwide_peaks_mean),\n",
    "                \"Val Epoch-Mean Batch-Max Max Deviation in Decibels: {:.4f}..\".format(val_batchwide_peaks_mean),\n",
    "                \"Val Epoch-Max Batch-Max Max Deviation in Decibels: {:.4f}\".format(val_epochwide_peak),\n",
    "            )\n",
    "            \n",
    "        if n_e < epochs:\n",
    "            # quota management\n",
    "            if allotted_seconds_left <= dur_last_epoch_s:\n",
    "                print(\"🛑 TIME 🛑\")\n",
    "                break\n",
    "            # early stop\n",
    "            if recent_i_epoch_at_lowest_val_loss >= (min_epochs_train_until_consider_early_stop-1) and i_e - recent_i_epoch_at_lowest_val_loss >= min_epochs_train_until_consider_early_stop and n_e / (1+recent_i_epoch_at_lowest_val_loss) >= 2:\n",
    "                print(\"🛑 EARLY STOP = validation loss didn’t decrease for too long 🛑\")\n",
    "                break\n",
    "                \n",
    "        dur_last_epoch_s = dur_this_epoch_s\n",
    "\n",
    "    history = { 'n_epochs': n_e, 'learning_rate': per_epoch_metrics['lr'], 'train_loss': per_epoch_metrics['train_loss'], 'val_loss': per_epoch_metrics['val_loss'], 'deep_weights':per_epoch_metrics['deep_weights'],\n",
    "                'error_peak_item': per_epoch_metrics['val_means_of_items_peak_errors'],'error_peak_batch': per_epoch_metrics['val_means_of_batches_peak_errors'],'error_peak_epoch': per_epoch_metrics['val_epochs_peak_errors'] }\n",
    "    print('▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬')\n",
    "    print('Total time: {:.3f} m'.format((time.time() - fit_time) / 60))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecbf15b",
   "metadata": {
    "id": "FjNcoo7dqu1B",
    "papermill": {
     "duration": 0.006491,
     "end_time": "2024-05-25T15:01:15.386707",
     "exception": false,
     "start_time": "2024-05-25T15:01:15.380216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Plots Loss + Accuracy Metrics Throughout Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0088eb4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:01:15.401113Z",
     "iopub.status.busy": "2024-05-25T15:01:15.400821Z",
     "iopub.status.idle": "2024-05-25T15:01:15.413106Z",
     "shell.execute_reply": "2024-05-25T15:01:15.412298Z"
    },
    "id": "Q8_EKAotO3Ul",
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.021838,
     "end_time": "2024-05-25T15:01:15.415145",
     "exception": false,
     "start_time": "2024-05-25T15:01:15.393307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    x = 1 + np.arange(0,history['n_epochs'])\n",
    "    \n",
    "    fig,(ax1,ax2,ax3) = plt.subplots(3, 1, sharex=True, figsize=(12, 12))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    def plot_loss():\n",
    "    \n",
    "        ax1.plot(x, history['val_loss'], label='val', marker='o')\n",
    "        ax1.plot(x, history['train_loss'], label='train', marker='o')\n",
    "        \n",
    "        i_least_val_loss = np.argmin(history['val_loss'])\n",
    "        x_least_val_loss = x[i_least_val_loss]\n",
    "        ax1.plot(x_least_val_loss,history['val_loss'][i_least_val_loss],'ro')\n",
    "\n",
    "        ax1.set_title('Loss per Epoch')\n",
    "        ax1.set_ylabel('MSE') # Modified Weighted MSE Loss\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.legend(), ax1.grid()\n",
    "    \n",
    "    def plot_inaccuracy():\n",
    "        \n",
    "        ax2.plot(x, history['error_peak_item'], label='item-max|batch-mean', marker='o')\n",
    "        ax2.plot(x, history['error_peak_batch'], label='batch-max|epoch-mean', marker='o')\n",
    "        ax2.plot(x, history['error_peak_epoch'], label='epoch-max', marker='o')\n",
    "\n",
    "        ax2.set_title('Peak Inaccuracy (dB) per Epoch')\n",
    "        ax2.set_ylabel('dB Off Target')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.legend(), ax2.grid()\n",
    "        \n",
    "    def plot_learnrate():\n",
    "        \n",
    "        ax3.plot(x, history['learning_rate'], label='learn rate', marker='o')\n",
    "        \n",
    "        ax3.set_title('Mean Learn Rate per Epoch')\n",
    "        ax3.set_ylabel('Learn Rate')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.grid()\n",
    "        \n",
    "        ax4 = ax3.twinx()  # instantiates a second Axes that shares the same x-axis\n",
    "        #color = 'tab:blue'\n",
    "        ax4.set_ylabel('Intermediate Outputs’ Contributions to Total Loss')  # we already handled the x-label with ax1\n",
    "        ax4.plot(x, history['deep_weights'][0], label='4th tier = 8x64')\n",
    "        ax4.plot(x, history['deep_weights'][1], label='3rd tier = 16x256')\n",
    "        ax4.plot(x, history['deep_weights'][2], label='2nd tier = 32x512')\n",
    "        ax4.legend(loc='upper right')\n",
    "        \n",
    "        ax3.legend(loc='upper left')\n",
    "        \n",
    "    plot_loss()\n",
    "    plot_inaccuracy()\n",
    "    plot_learnrate()\n",
    "\n",
    "    plt.savefig('./weight/charts.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ecc3aa",
   "metadata": {
    "papermill": {
     "duration": 0.006214,
     "end_time": "2024-05-25T15:01:15.427740",
     "exception": false,
     "start_time": "2024-05-25T15:01:15.421526",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Datasets + Runtime Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f817ebd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:01:15.441740Z",
     "iopub.status.busy": "2024-05-25T15:01:15.441502Z",
     "iopub.status.idle": "2024-05-25T15:01:15.494629Z",
     "shell.execute_reply": "2024-05-25T15:01:15.493824Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.062788,
     "end_time": "2024-05-25T15:01:15.496737",
     "exception": false,
     "start_time": "2024-05-25T15:01:15.433949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArraysChainer():\n",
    "    def __init__(self,arrays):\n",
    "        self.arrays = arrays\n",
    "        self.len_total = int(np.sum([len(a) for a in self.arrays],dtype=np.int64))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len_total\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        which_array = 0\n",
    "        subtracted_idx = idx\n",
    "        while subtracted_idx >= (len_here := len(self.arrays[which_array])):\n",
    "            subtracted_idx -= len_here\n",
    "            which_array += 1\n",
    "        return self.arrays[which_array][subtracted_idx]\n",
    "\n",
    "class NumpyMemMapPairsChainsDataset(Dataset):\n",
    "    def __init__(self, x_chainers:tuple, y_chainer:ArraysChainer, dtype=np.float32): # , _device=device\n",
    "        x1_chainer,x2_chainer,x3_chainer = x_chainers\n",
    "        self.x1 = x1_chainer\n",
    "        self.x2 = x2_chainer\n",
    "        self.x3 = x3_chainer\n",
    "        self.y = y_chainer\n",
    "        assert len(self.x1) == len(self.y)\n",
    "        assert len(self.x2) == len(self.y)\n",
    "        assert len(self.x3) == len(self.y)\n",
    "        self.dtype = dtype\n",
    "        # self._device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            (\n",
    "                torch.from_numpy(self.x1[idx].astype(self.dtype)),\n",
    "                torch.from_numpy(self.x2[idx].astype(self.dtype)),\n",
    "                torch.from_numpy(self.x3[idx].astype(self.dtype)),\n",
    "            ),\n",
    "            torch.from_numpy(self.y[idx].astype(self.dtype)),\n",
    "        )\n",
    "    \n",
    "class AugmentationDatasetWrapper(Dataset):\n",
    "    # https://stackoverflow.com/a/74921042\n",
    "    def __init__(self, _train_subset, view_augmentation_multiplier=2, flip_augmentation_multiplier=0.25,layers_augmentation_multiplier=0.2):\n",
    "        assert view_augmentation_multiplier == int(view_augmentation_multiplier)\n",
    "        \n",
    "        self.base_set = _train_subset\n",
    "        \n",
    "        self.n_samples_per_base_item = 1 + view_augmentation_multiplier\n",
    "        \n",
    "        self._l = len(self.base_set)\n",
    "        self.w_crop_ratio = 1/2\n",
    "        self.w_bay_per_x = np.array([2*64,2*32,2*16])\n",
    "        assert np.all(self.w_bay_per_x * self.w_crop_ratio == np.floor(self.w_bay_per_x * self.w_crop_ratio))\n",
    "        self.w_bay_y_xmatch_index = 1\n",
    "        coarsest_res = min(self.w_bay_per_x)\n",
    "        shiftmax_coarsest_res = coarsest_res * (1-self.w_crop_ratio)\n",
    "        shifts_coarsest_res = np.arange(shiftmax_coarsest_res)\n",
    "        self.views_positions = [np.array([s * res / coarsest_res for res in self.w_bay_per_x]) for s in shifts_coarsest_res]\n",
    "        self.views_positions = np.array([poses.astype(int) for poses in self.views_positions if np.all((poses - poses.astype(int)) == 0)])\n",
    "        wholenum_shifts_coarsest_res = self.views_positions[:,-1]\n",
    "        self.probability_per_pos_per_remainder = [\n",
    "            (\n",
    "                _p := (norm.pdf(\n",
    "                    wholenum_shifts_coarsest_res / shiftmax_coarsest_res,\n",
    "                    loc=_i / (self.n_samples_per_base_item - 1),\n",
    "                    scale=1 / (self.n_samples_per_base_item - 1) / 2,\n",
    "                ) if self.n_samples_per_base_item > 1 else np.ones((len(self.views_positions),),dtype=np.float64))\n",
    "            )\n",
    "            / np.sum(_p)\n",
    "            for _i in np.arange(self.n_samples_per_base_item)\n",
    "        ]\n",
    "\n",
    "        \n",
    "        self.augmented_main_len = self._l * self.n_samples_per_base_item\n",
    "        self.augmentation_extra_len = int(self.augmented_main_len * flip_augmentation_multiplier)\n",
    "        self.augmented_nolayers_len = self.augmented_main_len + self.augmentation_extra_len\n",
    "        self.augmented_layers_len = int(self.augmented_nolayers_len * layers_augmentation_multiplier)\n",
    "        \n",
    "        octaves_below_nyquist = torch.log2((torch.arange(1, 1 + 1024) + 1.0 / 2.0) / 1024).repeat((32, 1))\n",
    "        self.spect_cascade_masks = (\n",
    "            torch.clamp(octaves_below_nyquist,min=-2,max=-1)+2,\n",
    "            (1-(torch.clamp(octaves_below_nyquist,min=-2,max=-1)+2)) * (torch.clamp(octaves_below_nyquist,min=-4,max=-3)+4),\n",
    "            1-(torch.clamp(octaves_below_nyquist,min=-4,max=-3)+4),\n",
    "        )\n",
    "        \n",
    "        self.seed = 0\n",
    "        \n",
    "        self.epoch()\n",
    "        \n",
    "    def epoch(self):\n",
    "        self.seed = int(time.time() * np.random.random())\n",
    "        \n",
    "        base_indices = np.arange(self._l)\n",
    "        if self.augmentation_extra_len <= self._l:\n",
    "            self.augmentation_map = np.random.choice(base_indices,size=self.augmentation_extra_len,replace=False)\n",
    "            return\n",
    "        augmentation_map_unique = base_indices\n",
    "        np.random.shuffle(augmentation_map_unique)\n",
    "        self.augmentation_map = np.resize(augmentation_map_unique,self.augmentation_extra_len)\n",
    "        \n",
    "    def rancrop(self,x,y,idx_getter):\n",
    "        x1,x2,x3 = x\n",
    "        x1 = torch.unsqueeze(x1,dim=0)\n",
    "        x2 = torch.unsqueeze(x2,dim=0)\n",
    "        x3 = torch.unsqueeze(x3,dim=0)\n",
    "        _y = torch.unsqueeze(y,dim=0)\n",
    "        \n",
    "        if self.n_samples_per_base_item <= 1:\n",
    "            which_shiftsset = (np.random.default_rng(self.seed + idx_getter + 0)).integers(0,len(self.views_positions)-1)\n",
    "        \n",
    "        else:\n",
    "            modulo = idx_getter % self.n_samples_per_base_item\n",
    "            #print(self.probability_per_pos_per_remainder[0].shape)\n",
    "            which_shiftsset = (np.random.default_rng(self.seed + idx_getter + 1)).choice(np.arange(len(self.views_positions)),p=self.probability_per_pos_per_remainder[modulo])\n",
    "        \n",
    "        shifts = self.views_positions[which_shiftsset]\n",
    "        \n",
    "        y_fullsize = _y[:,shifts[self.w_bay_y_xmatch_index]:shifts[self.w_bay_y_xmatch_index]+int(self.w_bay_per_x[self.w_bay_y_xmatch_index]*self.w_crop_ratio),:]\n",
    "        #print(y_fullsize.shape)\n",
    "        return ((x1[:,shifts[0]:shifts[0]+int(self.w_bay_per_x[0]*self.w_crop_ratio),:],\n",
    "                x2[:,shifts[1]:shifts[1]+int(self.w_bay_per_x[1]*self.w_crop_ratio),:],\n",
    "                x3[:,shifts[2]:shifts[2]+int(self.w_bay_per_x[2]*self.w_crop_ratio),:]),\n",
    "                y_fullsize\n",
    "               )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.augmented_nolayers_len + self.augmented_layers_len\n",
    "\n",
    "    def __getitem__(self, idx,is_intermediate=False):\n",
    "        layers = idx >= self.augmented_nolayers_len\n",
    "        flip = (not layers) and idx >= self.augmented_main_len\n",
    "        if layers:\n",
    "            x_cropped,y_cropped = self.dual_layer_getitem(idx)\n",
    "        else:\n",
    "            base_idx = self.augmentation_map[idx-self.augmented_main_len] if flip else idx // self.n_samples_per_base_item\n",
    "            x_cropped,y_cropped = self.rancrop(*self.base_set[base_idx],idx)\n",
    "        if is_intermediate:\n",
    "            return x_cropped,y_cropped\n",
    "        if flip:\n",
    "            x_cropped = tuple([torch.flip(x_item,(1,)) for x_item in x_cropped])\n",
    "            y_cropped = torch.flip(y_cropped,(1,))\n",
    "        y_cropped = (\n",
    "                    F.interpolate(torch.unsqueeze(y_cropped,dim=0), size=(8,64),\n",
    "                                          mode='bilinear', align_corners=False)[0],\n",
    "                    F.interpolate(torch.unsqueeze(y_cropped,dim=0), size=(16,256),\n",
    "                                          mode='bilinear', align_corners=False)[0],\n",
    "                    F.interpolate(torch.unsqueeze(y_cropped,dim=0), size=(32,512),\n",
    "                                          mode='bilinear', align_corners=False)[0],\n",
    "                    y_cropped,\n",
    "                )\n",
    "        return x_cropped,y_cropped\n",
    "    \n",
    "    def spect_cascade(self,Y1024,Y2048,Y4096):\n",
    "        return Y1024 * self.spect_cascade_masks[0] + Y2048 * self.spect_cascade_masks[1] + Y4096 * self.spect_cascade_masks[2]\n",
    "    \n",
    "    def dual_layer_getitem(self,idx_getter):\n",
    "        to_amp = lambda dB : 10.0 ** (dB / 20.0)\n",
    "        from_amp = lambda a : 20.0 * torch.log10(a)\n",
    "        \n",
    "        pointee_idx_1 = (np.random.default_rng(self.seed + idx_getter + 2)).integers(0,self.augmented_nolayers_len)\n",
    "        pointee_idx_2 = (np.random.default_rng(self.seed + idx_getter + 3)).integers(0,self.augmented_nolayers_len)\n",
    "        pointee_1_x,pointee_1_y = self.__getitem__(pointee_idx_1,True)\n",
    "        pointee_2_x,pointee_2_y = self.__getitem__(pointee_idx_2,True)\n",
    "        Xs = []\n",
    "        Ys = []\n",
    "        mix_fac = (np.random.default_rng(self.seed + idx_getter + 4)).uniform(0,1)\n",
    "        for p1x,p2x in zip(pointee_1_x,pointee_2_x):\n",
    "            amp_1 = to_amp(p1x)\n",
    "            amp_2 = to_amp(p2x)\n",
    "            amp_total = (1-mix_fac) * amp_1 + mix_fac * amp_2\n",
    "            Xs.append(from_amp(amp_total))\n",
    "            \n",
    "            #print(p1x.shape)\n",
    "            \n",
    "            p1x_scaled = F.interpolate(torch.unsqueeze(p1x,dim=0), size=pointee_1_y.shape[1:],\n",
    "                                          mode='bilinear', align_corners=False)[0]\n",
    "            p2x_scaled = F.interpolate(torch.unsqueeze(p2x,dim=0), size=pointee_2_y.shape[1:],\n",
    "                                          mode='bilinear', align_corners=False)[0]\n",
    "            amp_1 = to_amp(p1x_scaled)\n",
    "            amp_2 = to_amp(p2x_scaled)\n",
    "            amp_total = (1-mix_fac) * amp_1 + mix_fac * amp_2\n",
    "            \n",
    "            contrib_1 = (1-mix_fac) * amp_1 / amp_total\n",
    "            contrib_2 = mix_fac * amp_2 / amp_total\n",
    "            Ys.append(from_amp(contrib_1 * to_amp(pointee_1_y) + contrib_2 * to_amp(pointee_2_y)))\n",
    "            \n",
    "        #print(pointee_1_x[0])\n",
    "        #print(pointee_2_x[0])\n",
    "        #print(pointee_1_y)\n",
    "        #print(pointee_2_y)\n",
    "        #print(self.spect_cascade(*Ys))\n",
    "            \n",
    "        return tuple(Xs),self.spect_cascade(*Ys)\n",
    "    \n",
    "def load_train_data(filter=''):\n",
    "    FOLDER = '/kaggle/input/'\n",
    "    FILENAME_STEM = 'train_'\n",
    "    specific_filter = (filter + '_') if len(filter) > 0 else filter\n",
    "    X1_files_outoforder = glob.glob(FOLDER+FILENAME_STEM+f'x1_{specific_filter}*.npy')\n",
    "    X2_files_outoforder = glob.glob(FOLDER+FILENAME_STEM+f'x2_{specific_filter}*.npy')\n",
    "    X3_files_outoforder = glob.glob(FOLDER+FILENAME_STEM+f'x3_{specific_filter}*.npy')\n",
    "    Y_files_outoforder = glob.glob(FOLDER+FILENAME_STEM+f'y_{specific_filter}*.npy')\n",
    "    \n",
    "    files_pairs = [] # to list to reuse the iterator\n",
    "    for yf in Y_files_outoforder:\n",
    "        correspond_x1f = yf.replace(FILENAME_STEM+'y',FILENAME_STEM+'x1')\n",
    "        correspond_x2f = yf.replace(FILENAME_STEM+'y',FILENAME_STEM+'x2')\n",
    "        correspond_x3f = yf.replace(FILENAME_STEM+'y',FILENAME_STEM+'x3')\n",
    "        assert correspond_x1f in X1_files_outoforder and correspond_x2f in X2_files_outoforder and correspond_x3f in X3_files_outoforder\n",
    "        files_pairs.append(((correspond_x1f,correspond_x2f,correspond_x3f),yf))\n",
    "    files_pairs = sorted(files_pairs, key=lambda a : os.path.getmtime(a[1]))\n",
    "    [print(_x1,_x2,_x3,_y) for (_x1,_x2,_x3),_y in files_pairs]\n",
    "    \n",
    "    IN_MEM_LIMIT_GiB = 15\n",
    "    IN_MEM_LIMIT_B = IN_MEM_LIMIT_GiB * 1024 ** 3\n",
    "    \n",
    "    mem_allocated_total_B = 0\n",
    "    \n",
    "    def load_to_mem_if_room(_fpath):\n",
    "        nonlocal mem_allocated_total_B\n",
    "        if mem_allocated_total_B >= IN_MEM_LIMIT_B:\n",
    "            return np.load(_fpath, mmap_mode='r')\n",
    "        loaded_to_ram = np.load(_fpath)\n",
    "        mem_allocated_total_B += loaded_to_ram.nbytes\n",
    "        return loaded_to_ram\n",
    "    \n",
    "    X1_arrays = []\n",
    "    X2_arrays = []\n",
    "    X3_arrays = []\n",
    "    Y_arrays = []\n",
    "    for (x1f,x2f,x3f),yf in files_pairs:\n",
    "        Y_arrays.append(load_to_mem_if_room(yf))\n",
    "        X1_arrays.append(load_to_mem_if_room(x1f))\n",
    "        X2_arrays.append(load_to_mem_if_room(x2f))\n",
    "        X3_arrays.append(load_to_mem_if_room(x3f))\n",
    "    \n",
    "    X1_all = ArraysChainer(X1_arrays)\n",
    "    X2_all = ArraysChainer(X2_arrays)\n",
    "    X3_all = ArraysChainer(X3_arrays)\n",
    "    Y_all = ArraysChainer(Y_arrays)\n",
    "    \n",
    "    return NumpyMemMapPairsChainsDataset((X1_all,X2_all,X3_all),Y_all) # dtype=np.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286b6b1",
   "metadata": {
    "id": "BEItKi48q1x-",
    "papermill": {
     "duration": 0.00624,
     "end_time": "2024-05-25T15:01:15.509416",
     "exception": false,
     "start_time": "2024-05-25T15:01:15.503176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loads Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b191746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:01:15.523542Z",
     "iopub.status.busy": "2024-05-25T15:01:15.523291Z",
     "iopub.status.idle": "2024-05-25T15:02:05.366439Z",
     "shell.execute_reply": "2024-05-25T15:02:05.365471Z"
    },
    "id": "I5dv5lXBPOGU",
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 49.860647,
     "end_time": "2024-05-25T15:02:05.376687",
     "exception": false,
     "start_time": "2024-05-25T15:01:15.516040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.device('cpu'):\n",
    "\n",
    "    dataset = load_train_data() # filter='130'\n",
    "    n_pairs = len(dataset)\n",
    "\n",
    "    generator1 = torch.Generator(device='cpu').manual_seed(42) # prevents data leakage artificially lowering val loss when resumes\n",
    "    VAL_SIZE_PROPORTION = np.tanh(np.sqrt(10**3/(10**3+np.e*n_pairs)))/2\n",
    "    print(VAL_SIZE_PROPORTION)\n",
    "    TRAIN_SET_COUNT_INFLATION_RATIO = 3\n",
    "    VAL_SIZE_PROPORTION_BEFORE_INFLATE_TRAIN = -(TRAIN_SET_COUNT_INFLATION_RATIO*VAL_SIZE_PROPORTION)/(VAL_SIZE_PROPORTION-TRAIN_SET_COUNT_INFLATION_RATIO*VAL_SIZE_PROPORTION-1)\n",
    "    # x_train, x_val, y_train, y_val = train_test_split(loaded_all_X,loaded_all_Y,test_size=VAL_SIZE_PROPORTION)\n",
    "    train_dataset, val_dataset = random_split(dataset,[1-VAL_SIZE_PROPORTION_BEFORE_INFLATE_TRAIN,VAL_SIZE_PROPORTION_BEFORE_INFLATE_TRAIN],generator=generator1)\n",
    "    train_dataset = AugmentationDatasetWrapper(train_dataset)\n",
    "    val_dataset = AugmentationDatasetWrapper(val_dataset,view_augmentation_multiplier=0)\n",
    "\n",
    "    BATCH_SIZE_TRAIN = 32\n",
    "    \"\"\"8 if n_pairs < 250 else (\n",
    "        16 if n_pairs < 1000 else (\n",
    "            32 if n_pairs < 75000 else (\n",
    "                64 if n_pairs < 1000000 else 128)))\"\"\"\n",
    "    BATCH_SIZE_VAL = 32 # recommendation = as big as hardware can handle | but smaller performed faster in tests with P100\n",
    "\n",
    "    N_LOADERWORKERS = 4\n",
    "    # train_set = np.array([x_train,y_train],dtype=np.float32) # np.swapaxes([x_train,y_train],0,1)\n",
    "    # train_dataset = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True, drop_last=True,\n",
    "        num_workers=N_LOADERWORKERS, pin_memory=True, persistent_workers=False, # https://discuss.pytorch.org/t/selective-augmentation-modifying-dataloader/108304/2\n",
    "        #device='cpu',\n",
    "    )\n",
    "    # val_set = np.array([x_val,y_val],dtype=np.float32) # np.swapaxes([x_val,y_val],0,1)\n",
    "    # val_dataset = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\n",
    "    valid_loader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE_VAL, shuffle=True, drop_last=True,\n",
    "        num_workers=N_LOADERWORKERS, pin_memory=True, persistent_workers=False, # https://discuss.pytorch.org/t/selective-augmentation-modifying-dataloader/108304/2\n",
    "        #device='cpu',\n",
    "    )\n",
    "\n",
    "    len_train = len(train_dataset)\n",
    "    len_val = len(val_dataset)\n",
    "    print(len_train)\n",
    "    print(len_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e8da2",
   "metadata": {
    "papermill": {
     "duration": 0.006886,
     "end_time": "2024-05-25T15:02:05.390722",
     "exception": false,
     "start_time": "2024-05-25T15:02:05.383836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run\n",
    "\n",
    "Resumes Training If Checkpoint Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b76a41c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:02:05.406857Z",
     "iopub.status.busy": "2024-05-25T15:02:05.406089Z",
     "iopub.status.idle": "2024-05-25T19:42:18.160656Z",
     "shell.execute_reply": "2024-05-25T19:42:18.159609Z"
    },
    "id": "Vvd9TZQAPle5",
    "papermill": {
     "duration": 16812.766271,
     "end_time": "2024-05-25T19:42:18.164224",
     "exception": false,
     "start_time": "2024-05-25T15:02:05.397953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = DeeplySupervizedUnet() # smp.Unet(\"resnet34\", encoder_weights=None, activation=None)\n",
    "net = net.to(device)\n",
    "# net.half()\n",
    "# net = torch.compile(net)\n",
    "# net = nn.DataParallel(net)\n",
    "\n",
    "loss_function = nn.MSELoss() #CustomMSELoss()\n",
    "# loss_function = torch.compile(loss_function)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=1e-4, weight_decay=1e-3) # , amsgrad=True\n",
    "\n",
    "# https://discuss.pytorch.org/t/cant-use-cycliclr-with-adam/74820\n",
    "# https://medium.com/analytics-vidhya/cyclical-learning-rates-a922a60e8c04\n",
    "n_iters_per_epoch = len(train_loader)\n",
    "#print(n_iters_per_epoch)\n",
    "scheduler = optim.lr_scheduler.CyclicLR(\n",
    "    optimizer,\n",
    "    base_lr=5e-5,max_lr=5e-4,\n",
    "    step_size_up=int(2*n_iters_per_epoch),step_size_down=int(10*n_iters_per_epoch), # https://medium.com/analytics-vidhya/cyclical-learning-rates-a922a60e8c04\n",
    "    mode=\"triangular2\",\n",
    "    cycle_momentum=False\n",
    ")\n",
    "\n",
    "# 🟢\n",
    "IS_KERNEL_OFFLINE_RUN = True # 🟢\n",
    "# 🟢\n",
    "\n",
    "PRETRAINED_MODEL_PATH = '/kaggle/input/'\n",
    "DO_RESUME_FROM_CHECKPOINT = True\n",
    "MUST_RESUME_FROM_CHECKPOINT = True\n",
    "RESET_OPTIM_LR_SCHED = False\n",
    "# Resumes Training If Checkpoint Exists\n",
    "real_chkpnt_pth = PRETRAINED_MODEL_PATH if IS_KERNEL_OFFLINE_RUN else CHECKPOINT_FILE_PATH\n",
    "if (True if MUST_RESUME_FROM_CHECKPOINT else DO_RESUME_FROM_CHECKPOINT) and os.path.isfile(real_chkpnt_pth):\n",
    "    checkpoint = torch.load(real_chkpnt_pth)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if not RESET_OPTIM_LR_SCHED:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    print(\"⏯️ RESUMES from CHECKPOINT ⏯️\")\n",
    "else:\n",
    "    print(\"▶️ TRAINS from SCRATCH ▶️\")\n",
    "    if MUST_RESUME_FROM_CHECKPOINT:\n",
    "        raise Exception(\"💥 checkpoint file not found 💥\")\n",
    "\n",
    "HOURS_TO_TRAIN = 5\n",
    "# KNOWN_DUR_SECONDS_EACH_EPOCH = 49\n",
    "# n_epochs = int(HOURS_TO_TRAIN * 60 ** 2 / KNOWN_DUR_SECONDS_EACH_EPOCH)\n",
    "n_epochs = 100\n",
    "\n",
    "#with autograd.detect_anomaly():\n",
    "history = train_and_val(n_epochs, net, train_loader, len_train, valid_loader, len_val, loss_function, optimizer, scheduler,\n",
    "                        intermediates_contributions_decays_per_epoch = np.array([0.5**(1/10),0.5**(1/20),0.5**(1/30)],dtype=np.float32),\n",
    "                        device=device,\n",
    "                        time_limit_hrs=HOURS_TO_TRAIN,\n",
    "                        use_amp=False,\n",
    "                       )\n",
    "\n",
    "plot_metrics(history)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "t01ghZwZqXML",
    "Vhblc41Kqkr6",
    "FxPmA6uCqtAS",
    "FjNcoo7dqu1B",
    "BEItKi48q1x-"
   ],
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4996472,
     "sourceId": 8514742,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 44728,
     "sourceId": 55410,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16874.715502,
   "end_time": "2024-05-25T19:42:19.830185",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-25T15:01:05.114683",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
